{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Project 1: Social Sentiment Analysis for Stock Prediction\n",
        "\n",
        "# Introduction & Project Overview\n",
        "\n",
        "## Purpose\n",
        "The primary objective of this project is show how sentiment from social media (e.g., Reddit) and news articles can correlate with financial data (e.g., stock prices). This could help identify whether shifts in sentiment (positive/negative) have a measurable impact on stock price movements, which is crucial for traders, investors, or analysts who seek to incorporate alternative data sources into their decision-making processes.\n",
        "\n",
        "**Important Note:** This project is aimed at exploring potential correlations between stock price movements and market sentiment.  It does not seek to establish causation or build a predictive model. The analysis will focus on identifying any statistical relationships between these variables, rather than determining if one directly causes the other.\n",
        "\n",
        "## Skills Showcased\n",
        "This project showcases my skills in data aggregation, cleaning, transformation, and merging datasets from different sources, as well as my proficiency in Python, SQL; and ability to work with PowerBI.\n",
        "\n",
        "## Selection of Pharma & Defence Stocks\n",
        "Pharmaceutical and defense sectors were selected due to their sensitivity to public perception and regulatory changes, making them ideal candidates for sentiment-based analysis. These sectors frequently experience significant/volatile price movements influenced by news coverage and social discussions, providing strong datasets for predictive analysis.\n",
        "\n",
        "## Data Sources Selection\n",
        "\n",
        "### Reddit\n",
        "Reddit was chosen due to its popularity, extensive discussion threads, and niche-focused subreddits that specifically cover financial news, investment strategies, and sector-specific discussions (e.g., pharma and defense stocks).\n",
        "\n",
        "### NEWSAPI\n",
        "NEWSAPI provides access to diverse, credible, and timely news sources globally. It allows for comprehensive collection of articles relevant to the target companies, enhancing the quality and depth of the sentiment analysis.\n",
        "\n",
        "### Twitter (Dropped)\n",
        "Originally, Twitter (X) was intended to be a primary data source. However, due to recent API limitations (restrictive access, high costs, and limitations on the volume of data retrieval), Twitter was dropped to ensure the feasibility, and sustainability of the project.\n",
        "\n",
        "## Notebook Objectives\n",
        "This notebook documents the step-by-step processes implemented during each project phase. Each phase is explained through relevant outcomes.\n",
        "\n",
        "\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "ZaNYykAY0-qI"
      },
      "id": "ZaNYykAY0-qI"
    },
    {
      "cell_type": "markdown",
      "id": "8fac32c6-c706-496c-9306-a4deee314e7f",
      "metadata": {
        "id": "8fac32c6-c706-496c-9306-a4deee314e7f"
      },
      "source": [
        "\n",
        "# Phase 1: Data Collection\n",
        "\n",
        "## **Summary of Accomplishments:**\n",
        "\n",
        "* Collected news articles for Pfizer, Moderna, Lockheed Martin, and Raytheon using the NewsAPI.\n",
        "\n",
        "* Scraped social media data from finance-related Reddit subreddits using PRAW.\n",
        "\n",
        "* Acquired historical OHLCV stock data for the same companies from Yahoo Finance, including 7-day and 14-day moving averages.\n",
        "\n",
        "* Ensured data integrity and consistency by implementing missing data handling (`dropna()`) and standardizing column formats.\n",
        "\n",
        "* Structured all collected data into a unified SQLite database (`sentiment.db`) using a defined schema (`schema.sql`) and a data loading script (`load_data.py`).\n",
        "\n",
        "* Secured sensitive information (API keys) using `.gitignore` and `.env` files.\n",
        "\n",
        "* Verified the integrity and structure of the database using DB Browser for SQLite.\n",
        "\n",
        "\n",
        "## **Step 1: Data Acquisition**\n",
        "Raw data was collected from three primary sources: Reddit, NewsAPI, and Yahoo Finance.\n",
        "\n",
        "### **Reddit Data Collection**\n",
        "\n",
        "  * Posts were scraped from the following subreddits, known for discussions on finance, pharmaceuticals, and defense:\n",
        "\n",
        " ```\n",
        "  r/stocks\n",
        "  r/wallstreetbets\n",
        "  r/investing\n",
        "  r/biotechplays\n",
        "  r/defensestocks\n",
        "  r/biotech\n",
        "```\n",
        "\n",
        "  * The PRAW library was used to access the Reddit API. API access is configured via `.env`.\n",
        "\n",
        "  * Relevant fields, including title, body, score, number of comments, and timestamp, were extracted from each post.\n",
        "\n",
        "  * **Output:** `scripts/reddit_scraper.py` generates `data/reddit_posts.csv`.\n",
        "\n",
        "  * **Code Snippet:**\n",
        "\n",
        "  ``` python\n",
        "  if __name__ == \"__main__\":\n",
        "    try:\n",
        "        target_subreddits = ['stocks', 'wallstreetbets', 'investing', 'biotechplays', 'defensestocks', 'biotech']\n",
        "        # Defined date range\n",
        "        start_date = datetime(2025, 4, 11)\n",
        "        end_date = datetime(2025, 5, 13)\n",
        "        df = scrape_reddit(target_subreddits, start_date, end_date, post_limit=200) # Get more posts\n",
        "\n",
        "        if not df.empty:\n",
        "            print(df.head())\n",
        "```\n",
        "**Rationale:** Reddit provides a valuable source of real-time discussion and sentiment regarding specific stocks and market trends. The selected subreddits were chosen for their high concentration of relevant financial discourse. Subreddits were also chosen for their varied userbases, which may aid in tracking a wider breadth of sentiments.\n",
        "\n",
        "### **NewsAPI Scraper**\n",
        "News Artices are pulled using NewsAPI for the following companies:\n",
        "- Pfizer\n",
        "- Moderna\n",
        "- Lockheed Martin\n",
        "- Raytheon\n",
        "\n",
        "The script parses the title, description, pub. date, source, and URL. API access is configured via `.env`.\n",
        "\n",
        "***Output:***\n",
        "`scripts/newsapi_scraper.py` generates `data/news_articles.csv`\n",
        "\n",
        "* **Code Snippet:**\n",
        "``` python\n",
        "def fetch_news_single_page(company_name, from_date, to_date, page=1):\n",
        "    params = {\n",
        "        'q': company_name,          # Keyword to search for\n",
        "        'language': 'en',\n",
        "        'pageSize': 100,            # Max n of results per page\n",
        "        'sortBy': 'publishedAt',    \n",
        "        'apiKey': API_KEY,          # API key stored in .env file      \n",
        "        'from': from_date.strftime('%Y-%m-%d'),\n",
        "        'to': to_date.strftime('%Y-%m-%d'),\n",
        "        'page': page\n",
        "```\n",
        " **Rationale:** NewsAPI aggregates news sources from a variety of sources and biases. It was chosen for both its comprehensiveness and ease-of-use.\n",
        "\n",
        "  News articles can significantly impact stock prices by providing information about company performance, market trends, and breaking events. These companies were selected as representative examples in the pharmaceutical and defense sectors.\n",
        "\n",
        "\n",
        "### **Stock Price Collection**\n",
        "\n",
        "Historical OHLCV (Open, High, Low, Close, Volume) data was downloaded from Yahoo Finance using the `yfinance` library for the same companies:\n",
        "- `PFE` = Pfizer\n",
        "- `MRNA` = Moderna\n",
        "- `LMT` = Lockheed Martin\n",
        "- `RTX` = Raytheon\n",
        "  \n",
        "    * In addition to the standard OHLCV data, 7-day and 14-day moving averages were calculated for the closing prices.\n",
        "\n",
        "***Output:***\n",
        "`scripts/stock_fetcher.py` generates `data/stock_data.csv`\n",
        "\n",
        "**Code Snippet:**\n",
        "\n",
        "``` python\n",
        "    # Download historical stock data\n",
        "    stock = yf.download(ticker, start=start_date, end=end_date)\n",
        "    stock.reset_index(inplace=True)\n",
        "\n",
        "    # Filter necessary columns and missing data\n",
        "    stock = stock[['Date', 'Open', 'High', 'Low', 'Close', 'Volume']].copy()\n",
        "    stock.dropna(inplace=True)\n",
        "\n",
        "    # Add moving averages, ticker column, and rearrange\n",
        "    stock['MA_7'] = stock['Close'].rolling(window=7).mean()\n",
        "    stock['MA_14'] = stock['Close'].rolling(window=14).mean()\n",
        "    stock['ticker'] = ticker\n",
        "    stock = stock[['ticker', 'Date', 'Open', 'High', 'Low', 'Close', 'MA_7', 'MA_14', 'Volume']]\n",
        "```\n",
        " **Rationale:** Historical stock price data provides the foundation for analyzing price trends and calculating technical indicators like moving averages, which can be used in trading strategies and financial analysis.\n",
        "\n",
        " This is necessary for my project in finding correlations between online sentiment and stock price movements.\n",
        "\n",
        "\n",
        "\n",
        "## **Step 2: Database Setup (SQLite)**\n",
        "A structured SQLite database (`sentiment.db`) was created to store the collected data. SQLite was chosen for its lightweight nature, ease of use, and suitability for this project's data volume.\n",
        "\n",
        "* The database schema was defined in `scripts/schema.sql`. This file specifies the tables and their columns, ensuring data integrity and organization.\n",
        "\n",
        "* The `scripts/database.py` script was used to create the database from the schema file. This script reads the SQL commands in `schema.sql` and executes them to generate the database.\n",
        "\n",
        "**Code Snippet (Illustrative, from `database.py`):**\n",
        "\n",
        "``` python\n",
        "def create_database_from_schema(schema_file, db_file):\n",
        "    \"\"\"\n",
        "    Creates an SQLite database from a given schema file.\n",
        "\n",
        "    Args:\n",
        "        schema_file (str): The path to the schema.sql file.\n",
        "        db_file (str): The name of the database file to create.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        with sqlite3.connect(db_file) as conn:\n",
        "            with open(schema_file, 'r') as f:\n",
        "                conn.executescript(f.read())\n",
        "            print(f\"Database file '{db_file}' created successfully from schema file '{schema_file}'.\")\n",
        "\n",
        "    except (...) # Here, errors were printed for debugging.\n",
        "\n",
        "```\n",
        "**Rationale:** Using a database ensures that the collected data is stored in a structured and easily queryable format, facilitating efficient data retrieval and analysis in subsequent phases. A schema definition promotes data consistency.\n",
        "\n",
        "The following tables are designed:\n",
        "- `reddit_posts`: Contains scraped Reddit content\n",
        "- `news_articles`: Contains article metadata from NEWSAPI\n",
        "- `stock_prices`: Contains historical OHLCV stock data & MAs for the selected companies\n",
        "\n",
        "\n",
        "### Creating the Database from Schema\n",
        "\n",
        "The `scripts/database.py` script reads the schema file and builds the SQLite database.\n",
        "\n",
        "**Output:**\n",
        "Creates `data/sentiment.db`\n",
        "\n",
        "## **Step 3: Data Loading**\n",
        "\n",
        "The data collected in Step 1 was loaded into the `sentiment.db` database using the `scripts/load_data.py` script.\n",
        "\n",
        "* This script reads the data from the CSV files generated in Step 1 (`reddit_posts.csv`, `news_articles.csv`, and `stock_data.csv`) and inserts it into the corresponding tables in the database (`reddit_posts`, `news_articles`, and `stock_prices`).\n",
        "\n",
        "* The `pandas` library was used to read the CSV files, and the `to_sql()` method was used to write the data to the database.\n",
        "\n",
        "**Code Snippet (Illustrative, from `load_data.py`):**\n",
        "``` python\n",
        "# Load data from CSV files into the database\n",
        "load_csv_to_db(os.path.join(\"data\", \"reddit_posts.csv\"), \"reddit_posts\")\n",
        "load_csv_to_db(os.path.join(\"data\", \"news_articles.csv\"), \"news_articles\")\n",
        "load_csv_to_db(os.path.join(\"data\", \"stock_data.csv\"), 'stock_prices')\n",
        "\n",
        "```\n",
        "**Note on importing OS:** By integrating predominantly `os.path.join()` throughout the project, I ensured that file paths were constructed in a way that is both reliable and compatible with various operating systems. This approach not only enhanced the robustness of the project but also contributed to its portability and ease of maintenance.\n",
        "\n",
        "**Tables ← CSV sources:**\n",
        "\n",
        "`reddit_posts` ← `data/reddit_posts.csv`\n",
        "\n",
        "`news_articles` ← `data/news_articles.csv`\n",
        "\n",
        "`stock_prices` ← `data/stock_data.csv`\n",
        "\n",
        "**Rationale:** This step centralizes all the collected data into the SQLite database, making it easier to manage and query for further analysis.\n",
        "\n",
        "\n",
        "## Final Note on Phase 1\n",
        "\n",
        "**Security Considerations:**\n",
        "\n",
        "* API keys for NewsAPI and Reddit were stored in a `.env` file.\n",
        "\n",
        "* The `.gitignore` file was configured to prevent the `.env` file from being committed to version control, ensuring that sensitive credentials are not exposed.\n",
        "\n",
        "**Verification:**\n",
        "\n",
        "* The contents of the `sentiment.db` database were verified using DB Browser for SQLite to ensure that the data was loaded correctly and the database schema was properly applied.\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Phase 2: Data Processing\n",
        "\n",
        "##**Summary of Accomplishments:**\n",
        "\n",
        "* Performed initial data cleaning and manipulation with `preprocess_data.py` using the `pandas` library.\n",
        "\n",
        "* Processed text using natural language processing techniques with `clean_sentiment.db.py`, using the `nltk` library.\n",
        "\n",
        "* Calculated sentiment scores using the VADER sentiment analysis tool with `calculate_sentiment.py`.\n",
        "\n",
        "* Stored all processed data, including sentiment scores, back into the SQLite database.\n",
        "\n",
        "##**Step 1: Initial Cleaning and Transformation**\n",
        "\n",
        "The `preprocess_data.py` script was created to perform the initial cleaning and transformation of the raw data. This script reads data from the database created in *phase 1*, performs data manipulation using pandas, then writes it back to the database.\n",
        "\n",
        "#### Data Transformations:\n",
        "**Reddit:**\n",
        "```python\n",
        "reddit_df['body'] = reddit_df['body'].fillna('')  #  Fills in missing body text\n",
        "reddit_df['combined_text'] = reddit_df['title'] + ' ' + reddit_df['body'] # Combines title and body to get new column 'combined_text' column\n",
        "reddit_df['created_at'] = pd.to_datetime(reddit_df['created_at']) # Datetime format for later analysis\n",
        "reddit_df.drop(columns=['title', 'body'], inplace=True) # Title and body are no longer needed\n",
        "reddit_df.reset_index(drop=True, inplace=True) #  reset index\n",
        "```\n",
        "*For the News data, similar transformations are undertaken, and can be found in the `data/preprocess_data.py` file.\n",
        "\n",
        "**Stocks:**\n",
        "\n",
        "```python\n",
        "# Date column converted to datetime format\n",
        "stock_df['date'] = pd.to_datetime(stock_df['date'])\n",
        "\n",
        "# Calculate daily percentage change, grouped by ticker\n",
        "stock_df['daily_return'] = stock_df.groupby('ticker')['close'].pct_change()\n",
        "\n",
        "```\n",
        "\n",
        "**Rationale:** This step prepares the data for subsequent analysis by addressing missing values, combining relevant text fields, standardizing date formats, and calculating a key financial metric (daily return) for the stock data.  Storing the cleaned data in new tables preserves the original data and makes it clear which data has been processed.\n",
        "\n",
        "##**Step 2: Text Data Processing**\n",
        "\n",
        "The `clean_sentiment.db.py` script processed the data using natural language processing techniques. This was necessary in order to calculate accurate sentiment scores using VADER in the next step.\n",
        "\n",
        "The script performs the following:\n",
        "\n",
        "* Converts the text to lowercase using `.lower()`.\n",
        "\n",
        "* Removes URLs and special characters using regular expressions `re.sub()`.\n",
        "\n",
        "* Tokenizes the text into individual words using `word_tokenize()`.\n",
        "\n",
        "* Removes stop words (common words like \"the\", \"and\", \"is\") using NLTK's stop word list.\n",
        "\n",
        "* Lemmatizes the words (reduce them to their base form) using NLTK's WordNet Lemmatizer.\n",
        "\n",
        "Data is then loaded into Pandas DataFrames, and written to the cleaned tables.\n",
        "\n",
        "**Code Snippet:**\n",
        " ```python\n",
        "def clean_text(text):\n",
        "    if pd.isna(text):\n",
        "        return \"\"\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text)\n",
        "    text = re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", text)\n",
        "    tokens = word_tokenize(text)\n",
        "    stop_words = set(stopwords.words(\"english\"))\n",
        "    filtered = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n",
        "    return \" \".join(filtered)\n",
        "```\n",
        "**Rationale:** This step removes noise and irrelevant information from the text data, standardizing it and improving the accuracy of the sentiment analysis in the next step.  Lemmatization helps to group words with similar meanings.  Retaining the original timestamps is essential for time-series analysis and merging with stock price data.\n",
        "\n",
        "##**Step 3: Sentiment Analysis**\n",
        "\n",
        "The `calculate_sentiment.py` script calculates sentiment scores for the cleaned text data using the VADER sentiment analysis tool. Vader combines the score of each word in the combined text to give an overall score for the text.\n",
        "\n",
        "The sentiment scores are then stored in the database.\n",
        "\n",
        "**Code snippet**\n",
        "\n",
        "```python\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "\n",
        "# Initialize VADER sentiment analyzer\n",
        "analyzer = SentimentIntensityAnalyzer()\n",
        "\n",
        "def get_vader_sentiment(text):\n",
        "    \"\"\"Calculates the VADER sentiment scores for the input text.\"\"\"\n",
        "    if pd.isna(text):\n",
        "        return {'neg': 0.0, 'neu': 0.0, 'pos': 0.0, 'compound': 0.0}\n",
        "    return analyzer.polarity_scores(text)\n",
        "```\n",
        "\n",
        "The `get_vader_sentiment()` function calculates the negative, neutral, positive, and compound sentiment scores for each text.  The compound score is stored in a new column (`reddit_sentiment` and `news_sentiment` for Reddit and News data, respectively).\n",
        "\n",
        "The processed DataFrames, now containing the sentiment scores, are written back to the `cleaned_reddit_posts` and `cleaned_news_articles` tables in the sentiment.db database.\n",
        "\n",
        "**Rationale:** This step quantifies the emotional tone of the text data, providing a numerical representation of market sentiment. VADER is well-suited for analyzing text from social media and news articles. Storing the sentiment scores in the database allows for easy integration with other data, such as stock prices, in subsequent analysis.\n"
      ],
      "metadata": {
        "id": "RnMzxQgiN6Cy"
      },
      "id": "RnMzxQgiN6Cy"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**NEXT STEPS IN POWERBI**\n",
        "\n",
        "**---UNDER CONSTRUCTION---**"
      ],
      "metadata": {
        "id": "ngk3f5koQ2XB"
      },
      "id": "ngk3f5koQ2XB"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.3"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}