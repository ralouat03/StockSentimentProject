{"cells":[{"cell_type":"code","source":[],"metadata":{"id":"3TDTICZzWNyp"},"id":"3TDTICZzWNyp","execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"8fac32c6-c706-496c-9306-a4deee314e7f","metadata":{"id":"8fac32c6-c706-496c-9306-a4deee314e7f"},"source":["# Project 1: Real-Time Social Sentiment Analysis for Stock Prediction\n","\n","# Introduction & Project Overview\n","\n","## Purpose\n","The primary objective of this project is to develop an end-to-end pipeline that leverages real-time social sentiment analysis to predict stock market movements. Specifically, it aims to analyze the sentiment from various online sources to forecast short-term trends in stock prices.\n","\n","## Selection of Pharma & Defence Stocks\n","Pharmaceutical and defense sectors were selected due to their sensitivity to public perception and regulatory changes, making them ideal candidates for sentiment-based analysis. These sectors frequently experience significant/volatile price movements influenced by news coverage and social discussions, providing strong datasets for predictive analysis.\n","\n","## Data Sources Selection\n","\n","### Reddit\n","Reddit was chosen due to its popularity, extensive discussion threads, and niche-focused subreddits that specifically cover financial news, investment strategies, and sector-specific discussions (e.g., pharma and defense stocks).\n","\n","### NEWSAPI\n","NEWSAPI provides access to diverse, credible, and timely news sources globally. It allows for comprehensive collection of articles relevant to the target companies, enhancing the quality and depth of the sentiment analysis.\n","\n","### Twitter (Dropped)\n","Originally, Twitter (X) was intended to be a primary data source. However, due to recent API limitations (restrictive access, high costs, and limitations on the volume of data retrieval), Twitter was dropped to ensure the feasibility, and sustainability of the project.\n","\n","## Notebook Objectives\n","This notebook documents the step-by-step processes implemented during each project phase. The current notebook specifically covers **Phase 1: Data Collection**. Each phase is explained and demonstrated through relevant code and outcomes.\n","\n","---\n","\n","# Phase 1: Data Collection\n","\n","## Summary of Accomplishments\n","- Set up Reddit and NewsAPI scrapers.\n","- Pulled and saved news and social data for Pfizer, Moderna, Lockheed Martin, and Raytheon.\n","- Collected historical OHLCV data using `yfinance`, including 7-day and 14-day moving averages.\n","- Implemented missing data cleaning (`dropna`) and column formatting.\n","- Updated file paths in the scripts to use relative paths, ensuring that the project can be easily moved or cloned to different environments without needing to adjust file paths manually.\n","- Structed all data into a unified SQLite database using `schema.sql` and `load_data.py`.\n","- Implemented `.gitignore` and `.env` files to keep API keys and credentials secure.\n","- Verified contents using DB Browser for SQLite.\n","\n","## Step 1: Setting up APIs\n","APIs were configured for Reddit (via PRAW) and NEWSAPI.\n","\n","## Reddit Scraper\n","Utilises PRAW to scrape posts from specific finance-related subreddits. This project specifically targets subreddits 'stocks', 'wallstreetbets', 'biotechplays', 'defensestocks', and 'biotech'.\n","\n","```python\n","# reddit_scraper.py\n","import praw\n","import pandas as pd\n","from datetime import datetime\n","import os\n","\n","# Load environment variables from .env file\n","from dotenv import load_dotenv\n","load_dotenv()\n","\n","# API credentials from .env file\n","API_ID = os.getenv('API_ID')\n","API_SECRET = os.getenv('API_SECRET')\n","AGENT = os.getenv('AGENT')\n","USSR = os.getenv('USSR')\n","PASS = os.getenv('PASS')\n","\n","# Check if Reddit credentials exist\n","if not all([API_ID, API_SECRET, AGENT, USSR, PASS]):\n","    raise ValueError(\"Missing one or more Reddit API credentials in .env file!\")\n","\n","reddit = praw.Reddit(\n","    client_id=API_ID,\n","    client_secret=API_SECRET,\n","    user_agent=AGENT,\n","    username=USSR,\n","    password=PASS\n","                     \n","# Extract relevant information from subreddits\n","def scrape_reddit(subreddits, post_limit=50):\n","    posts = []\n","    for sub in subreddits:\n","        subreddit = reddit.subreddit(sub)\n","        for post in subreddit.hot(limit=post_limit):\n","            posts.append({\n","                'subreddit': sub,\n","                'title': post.title,\n","                'body': post.selftext,\n","                'score': post.score,\n","                'comments': post.num_comments,\n","                'created_at': datetime.fromtimestamp(post.created_utc)\n","            })\n","    return pd.DataFrame(posts)\n","\n","# Main execution\n","if __name__ == \"__main__\":\n","    try:\n","        target_subreddits = ['stocks', 'wallstreetbets', 'investing', 'biotechplays', 'defensestocks', 'biotech']\n","        df = scrape_reddit(target_subreddits)\n","\n","        if not df.empty:\n","            print(df.head())\n","\n","            data_dir = os.path.join(os.path.dirname(os.path.abspath(__file__)), \"..\", \"data\")\n","            os.makedirs(data_dir, exist_ok=True)\n","            output_path = os.path.join(data_dir, \"reddit_posts.csv\")\n","\n","            df.to_csv(output_path, index=False)\n","            print(f\"Saved scraped posts to {output_path}\")\n","        else:\n","            print(\"No posts fetched. Check credentials or subreddit accessibility.\")\n","\n","    except Exception as e:\n","        print(f\"Error: {e}\")\n","\n","    input(\"Press Enter to exit...\")\n","  \n","```\n","\n","## NEWSAPI Scraper\n","Using NEWSAPI to collect news articles relevant to selected companies. This project specifically collects articles regarding Pfizer, Moderna, Lockheed Martin, and Raytheon.\n","\n","```python\n","import requests\n","import pandas as pd\n","from datetime import datetime\n","import time\n","import os\n","\n","API_KEY = os.getenv('API_KEY')\n","BASE_URL = os.getenv('BASE_URL')\n","\n","# Check if NewsAPI credentials exist\n","if not all([API_KEY, BASE_URL]):\n","    raise ValueError(\"Missing one or both NewsAPI credentials in .env file!\")\n","\n","# Fetches news article\n","def fetch_news(company_name):\n","    params = {\n","        'q': company_name,          # Keyword to search for\n","        'language': 'en',           \n","        'pageSize': 100,            # Max n of results per page\n","        'sortBy': 'publishedAt',    \n","        'apiKey': API_KEY\n","    }\n","\n","    r = requests.get(BASE_URL, params=params)\n","    data = r.json()\n","\n","    # Extract relevant info from articles\n","    articles = []\n","    for article in data.get('articles', []):\n","        articles.append({\n","            'company': company_name,\n","            'title': article['title'],\n","            'description': article['description'],\n","            'published_at': article['publishedAt'],\n","            'source': article['source']['name'],\n","            'url': article['url'],\n","            'scraped_at': datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n","        })\n","\n","    return pd.DataFrame(articles)\n","\n","# Main execution block\n","if __name__ == \"__main__\":\n","    companies = ['Pfizer', 'Moderna', 'Lockheed Martin', 'Raytheon']\n","    all_news = []\n","\n","    for company in companies:\n","        print(f\"Fetching news for {company}...\")\n","        df = fetch_news(company)\n","        all_news.append(df)\n","        time.sleep(1.5) # Small delay for API\n","\n","    result_df = pd.concat(all_news, ignore_index=True)\n","\n","     # Get the directory where the current script is located (data folder)\n","    script_dir = os.path.dirname(os.path.abspath(__file__))\n","    data_dir = os.path.join(script_dir, \"..\", \"data\")\n","    output_path = os.path.join(data_dir, \"news_articles.csv\")\n","\n","    result_df.to_csv(output_path, index=False)\n","    print(f\"News saved to {output_path}\")\n","```\n","\n","## Collecting Financial Data (Stock Prices)\n","To complete the data collection phase, I retrieved historical stock price data for the monitored companies, using the 'yfinance' library, pulling OHLC, Volume, and Date data from Yahoo Finance.\n","\n","```python\n","# stock_fetcher.py\n","\n","import yfinance as yf\n","import pandas as pd\n","import os  # Import the os module\n","\n","# Define target stock tickers and company names\n","companies = {\n","    'PFE': 'Pfizer',\n","    'MRNA': 'Moderna',\n","    'LMT': 'Lockheed Martin',\n","    'RTX': 'Raytheon'\n","}\n","\n","# Date range for historical stock data\n","start_date = '2022-01-01'\n","end_date = '2024-12-31'\n","\n","# Store all company data in a list\n","all_data = []\n","\n","for ticker, name in companies.items():\n","    print(f\" Fetching data for {name} ({ticker})...\")\n","\n","    # Download historical stock data\n","    stock = yf.download(ticker, start=start_date, end=end_date)\n","    stock.reset_index(inplace=True)\n","\n","    # Filter necessary columns and missing data\n","    stock = stock[['Date', 'Open', 'High', 'Low', 'Close', 'Volume']].copy()\n","    stock.dropna(inplace=True)\n","\n","    # Add moving averages, ticker column, and rearrange\n","    stock['MA_7'] = stock['Close'].rolling(window=7).mean()\n","    stock['MA_14'] = stock['Close'].rolling(window=14).mean()\n","    stock['ticker'] = ticker\n","    stock = stock[['ticker', 'Date', 'Open', 'High', 'Low', 'Close', 'MA_7', 'MA_14', 'Volume']]\n","\n","    # Rename columns to match database schema format (lowercase)\n","    stock.columns = ['ticker', 'date', 'open', 'high', 'low', 'close', 'ma_7', 'ma_14', 'volume']\n","\n","    all_data.append(stock)\n","\n","# Concatenate all company DataFrames\n","df = pd.concat(all_data, ignore_index=True)\n","\n","# Get the directory where the current script is located, and creates path for .csv\n","script_dir = os.path.dirname(os.path.abspath(__file__))\n","data_dir = os.path.join(script_dir, \"..\", \"data\")\n","output_path = os.path.join(data_dir, \"stock_data.csv\")\n","\n","# Save the combined DataFrame to a CSV file\n","df.to_csv(output_path, index=False)\n","print(f\"Stock data (with moving averages) successfully saved to {output_path}\")\n","\n","```\n","\n","## Step 2: Database Setup (SQLite)\n","Creating structured database tables to store collected data.\n","\n","The following tables are designed:\n","- `reddit_posts`: Contains scraped Reddit content\n","- `news_articles`: Contains article metadata from NEWSAPI\n","- `stock_prices`: Contains historical OHLCV stock data & MAs for the selected companies\n","\n","```python\n","# database.py\n","import sqlite3\n","import os\n","\n","def create_database_from_schema(schema_file, db_file):\n","    \"\"\"\n","    Creates an SQLite database from a given schema file.\n","\n","    Args:\n","        schema_file (str): The path to the schema.sql file.\n","        db_file (str): The name of the database file to create.\n","    \"\"\"\n","    try:\n","        with sqlite3.connect(db_file) as conn:\n","            with open(schema_file, 'r') as f:\n","                conn.executescript(f.read())\n","            print(f\"Database file '{db_file}' created successfully from schema file '{schema_file}'.\")\n","\n","    except sqlite3.Error as e:\n","        print(f\"An error occurred: {e}\")\n","    except FileNotFoundError:\n","        print(f\"Error: Schema file '{schema_file}' not found.\")\n","\n","if __name__ == \"__main__\":\n","    script_dir = os.path.dirname(os.path.abspath(__file__))\n","    schema_file = os.path.join(script_dir, 'schema.sql')\n","    db_file = os.path.join(os.path.dirname(script_dir), 'data', 'sentiment.db')\n","\n","    create_database_from_schema(schema_file, db_file)\n","```\n","\n","## Step 3: Loading CSV Data into Database\n","Data from the CSV files is loaded into the corresponding database tables.\n","\n","```python\n","# load_data.py\n","import sqlite3\n","import pandas as pd\n","import os\n","\n","def load_csv_to_db(csv_file, table_name):\n","    # Read the CSV file\n","    df = pd.read_csv(csv_file)\n","\n","    # Connect to the SQLite database\n","    conn = sqlite3.connect(os.path.join(\"data\", \"sentiment.db\"))\n","\n","    # Write the DataFrame to the SQL table\n","    df.to_sql(table_name, conn, if_exists='append', index=False)\n","\n","    # Close the connection\n","    conn.close()\n","    print(f\"Loaded data into {table_name} from {csv_file}\")\n","\n","# Load data from CSV files into the database\n","load_csv_to_db(os.path.join(\"data\", \"reddit_posts.csv\"), \"reddit_posts\")\n","load_csv_to_db(os.path.join(\"data\", \"news_articles.csv\"), \"news_articles\")\n","load_csv_to_db(os.path.join(\"data\", \"stock_data.csv\"), 'stock_prices')\n","```\n","## Final Note on Phase 1\n","All of the data gathered in Phase 1 is successfully sorted and centralised in `sentiment.db`, which was inspected using DB Browser."]},{"cell_type":"code","source":[],"metadata":{"id":"-djQUJ64xh4_"},"id":"-djQUJ64xh4_","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"id":"cd73007f-8ef1-44d0-bf7f-8ed78400b64e","metadata":{"id":"cd73007f-8ef1-44d0-bf7f-8ed78400b64e"},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.13.3"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}